import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pandas as pd
import os
import sys

# -------------------------- ä¸­æ–‡æ˜¾ç¤ºé…ç½® --------------------------
plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'SimHei', 'DejaVu Sans']  # ä¼˜å…ˆä½¿ç”¨çš„ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
# ------------------------------------------------------------------

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# è§£å†³matplotlibåœ¨Linuxæ— GUIçš„é—®é¢˜
import matplotlib
matplotlib.use('Agg')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from model import Transformer
from data_utils import load_iwslt_data, create_data_loaders, create_masks
from config import AblationConfig


class AblationStudy:
    """æ¶ˆèå®éªŒç±»ï¼šå¯¹æ¯”ä¸åŒæ¨¡å‹é…ç½®çš„æ€§èƒ½ï¼ˆå‚æ•°é‡ã€BLEUåˆ†æ•°ï¼‰"""

    def __init__(self):
        self.results = []  # å­˜å‚¨æ‰€æœ‰æ¶ˆèå®éªŒç»“æœ
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"æ¶ˆèå®éªŒè®¾å¤‡ï¼š{self.device}ï¼ˆ{torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}ï¼‰")

    def run_ablation(self, model_types=None):
        """è¿è¡Œæ¶ˆèå®éªŒï¼ˆé»˜è®¤æµ‹è¯•6ç§æ¨¡å‹é…ç½®ï¼‰"""
        # å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹ç±»å‹ï¼ˆå¯è‡ªå®šä¹‰å¢å‡ï¼‰
        if model_types is None:
            model_types = [
                "baseline",    # åŸºå‡†æ¨¡å‹ï¼ˆé»˜è®¤é…ç½®ï¼‰
                "small",       # å°æ¨¡å‹ï¼ˆå°ç»´åº¦+å°‘å±‚æ•°ï¼‰
                "large",       # å¤§æ¨¡å‹ï¼ˆå¤§ç»´åº¦+å¤šå±‚æ•°ï¼‰
                "no_dropout",  # æ— dropoutï¼ˆå…³é—­æ­£åˆ™åŒ–ï¼‰
                "more_heads",  # æ›´å¤šæ³¨æ„åŠ›å¤´
                "deep"         # æ›´æ·±å±‚æ•°
            ]

        print("=" * 70)
        print("å¼€å§‹Transformeræ¶ˆèå®éªŒ")
        print("=" * 70)
        print(f"æµ‹è¯•æ¨¡å‹é…ç½®ï¼š{', '.join(model_types)}")
        print(f"å®éªŒè®¾å¤‡ï¼š{self.device}")
        print(f"è®­ç»ƒè½®æ¬¡ï¼š8ï¼ˆæ¶ˆèå®éªŒå¿«é€ŸéªŒè¯ï¼‰")
        print("=" * 70 + "\n")

        # éå†æ¯ç§æ¨¡å‹é…ç½®ï¼Œè¿è¡Œå®éªŒ
        for idx, model_type in enumerate(model_types, 1):
            print(f"\nğŸ”¬ å®éªŒ {idx}/{len(model_types)}ï¼š{model_type}")
            print("-" * 50)

            try:
                # 1. åˆå§‹åŒ–å½“å‰æ¨¡å‹çš„é…ç½®ï¼ˆç»§æ‰¿AblationConfigï¼Œè‡ªåŠ¨ä¿®æ”¹å¯¹åº”å‚æ•°ï¼‰
                config = AblationConfig(model_type=model_type)
                config.epochs = 8  # æ¶ˆèå®éªŒå‡å°‘è®­ç»ƒè½®æ¬¡ï¼ˆå¿«é€Ÿå¯¹æ¯”ï¼‰
                config.batch_size = 32  # ç»Ÿä¸€æ‰¹æ¬¡å¤§å°ï¼ˆå…¬å¹³å¯¹æ¯”ï¼‰

                # 2. åŠ è½½æ•°æ®é›†ï¼ˆæ‰€æœ‰æ¨¡å‹å…±äº«åŒä¸€æ•°æ®é›†ï¼Œä¿è¯å¯¹æ¯”å…¬å¹³ï¼‰
                train_dataset, val_dataset, test_dataset, tokenizer = load_iwslt_data(config)
                train_loader, val_loader, test_loader = create_data_loaders(
                    train_dataset, val_dataset, test_dataset, config.batch_size
                )

                # 3. åˆå§‹åŒ–å½“å‰é…ç½®çš„æ¨¡å‹
                model = Transformer(
                    src_vocab_size=tokenizer.src_vocab_size,
                    tgt_vocab_size=tokenizer.tgt_vocab_size,
                    d_model=config.d_model,
                    nhead=config.nhead,
                    num_encoder_layers=config.num_encoder_layers,
                    num_decoder_layers=config.num_decoder_layers,
                    d_ff=config.dim_feedforward,
                    max_len=config.max_length,
                    dropout=config.dropout,
                    activation=config.activation
                ).to(self.device)

                # æ‰“å°å½“å‰æ¨¡å‹ä¿¡æ¯
                total_params = sum(p.numel() for p in model.parameters())
                print(f"æ¨¡å‹å‚æ•°é‡ï¼š{total_params:,}")
                print(f"æ¨¡å‹é…ç½®ï¼šd_model={config.d_model}, nhead={config.nhead}, "
                      f"layers={config.num_encoder_layers}/{config.num_decoder_layers}, "
                      f"d_ff={config.dim_feedforward}, dropout={config.dropout}")

                # 4. å¿«é€Ÿè®­ç»ƒå¹¶è·å–æœ€ä½³BLEUåˆ†æ•°
                best_bleu = self.fast_train(model, config, train_loader, val_loader, tokenizer)

                # 5. è®°å½•å®éªŒç»“æœ
                result = {
                    'model_type': model_type,
                    'parameters': total_params,
                    'd_model': config.d_model,
                    'nhead': config.nhead,
                    'num_encoder_layers': config.num_encoder_layers,
                    'num_decoder_layers': config.num_decoder_layers,
                    'd_ff': config.dim_feedforward,
                    'dropout': config.dropout,
                    'best_bleu': best_bleu,
                    'params_million': round(total_params / 1e6, 2)  # å‚æ•°é‡ï¼ˆç™¾ä¸‡ï¼‰
                }
                self.results.append(result)
                print(f"âœ… å®éªŒå®Œæˆï¼š{model_type} | BLEUåˆ†æ•°ï¼š{best_bleu:.2f}% | å‚æ•°é‡ï¼š{total_params:,}")

                # æ¸…ç†GPUç¼“å­˜ï¼ˆé¿å…å¤šæ¨¡å‹è®­ç»ƒæ˜¾å­˜æº¢å‡ºï¼‰
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            except Exception as e:
                print(f"âŒ å®éªŒå¤±è´¥ï¼š{model_type} | é”™è¯¯ä¿¡æ¯ï¼š{str(e)[:200]}")
                continue

        # 6. ä¿å­˜å®éªŒç»“æœå¹¶ç»˜å›¾
        self.save_results()
        self.plot_results()

        # 7. è¾“å‡ºå®éªŒæ€»ç»“
        self.print_summary()

        return self.results

    def fast_train(self, model, config, train_loader, val_loader, tokenizer):
        """å¿«é€Ÿè®­ç»ƒï¼ˆé€‚é…æ¶ˆèå®éªŒï¼Œç®€åŒ–è®­ç»ƒæµç¨‹ï¼Œèšç„¦æ€§èƒ½å¯¹æ¯”ï¼‰"""
        # æŸå¤±å‡½æ•°ï¼ˆå¿½ç•¥pad_tokenï¼‰
        criterion = nn.CrossEntropyLoss(ignore_index=0)
        # ä¼˜åŒ–å™¨ï¼ˆç»Ÿä¸€ä½¿ç”¨Adamï¼Œä¿è¯å¯¹æ¯”å…¬å¹³ï¼‰
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=config.learning_rate,
            betas=config.betas,
            eps=config.eps,
            weight_decay=config.weight_decay
        )
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=1, verbose=False
        )

        best_bleu = 0.0  # è®°å½•æœ€ä½³BLEUåˆ†æ•°

        for epoch in range(config.epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            for src, tgt in train_loader:
                src = src.to(self.device, non_blocking=True)
                tgt = tgt.to(self.device, non_blocking=True)

                # åˆ›å»ºæ©ç 
                src_mask, tgt_mask = create_masks(src, tgt)

                # å‰å‘ä¼ æ’­
                output = model(
                    src=src,
                    tgt=tgt[:, :-1],
                    src_mask=src_mask,
                    tgt_mask=tgt_mask[:, :, :-1, :-1]
                )

                # è®¡ç®—æŸå¤±
                output_dim = output.shape[-1]
                output = output.contiguous().view(-1, output_dim)
                tgt_output = tgt[:, 1:].contiguous().view(-1)
                loss = criterion(output, tgt_output)

                # åå‘ä¼ æ’­ + æ¢¯åº¦è£å‰ª
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)
                optimizer.step()

                train_loss += loss.item()

            # éªŒè¯é˜¶æ®µï¼ˆè®¡ç®—BLEUåˆ†æ•°ï¼‰
            model.eval()
            current_bleu = 0.0
            with torch.no_grad():
                for src, tgt in val_loader:
                    # é™åˆ¶æ‰¹é‡å¤§å°ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
                    if len(src) > 8:
                        src = src[:8]
                        tgt = tgt[:8]

                    src = src.to(self.device, non_blocking=True)
                    tgt = tgt.to(self.device, non_blocking=True)
                    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)

                    # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„BLEUåˆ†æ•°
                    batch_bleu = self.calculate_bleu_batch(model, src, tgt, src_mask, tokenizer)
                    current_bleu += batch_bleu
                    break  # ä»…éªŒè¯1ä¸ªæ‰¹æ¬¡ï¼ˆåŠ é€Ÿæ¶ˆèå®éªŒï¼‰

            # å¹³å‡BLEUåˆ†æ•°ï¼ˆå•æ‰¹æ¬¡ï¼‰
            current_bleu = current_bleu if current_bleu == 0 else current_bleu / 1
            # æ›´æ–°æœ€ä½³BLEUåˆ†æ•°
            if current_bleu > best_bleu:
                best_bleu = current_bleu

            # å­¦ä¹ ç‡è°ƒåº¦
            avg_train_loss = train_loss / len(train_loader)
            scheduler.step(avg_train_loss)

            # æ‰“å°å½“å‰epochè¿›åº¦
            print(f"Epoch [{epoch+1}/{config.epochs}] | è®­ç»ƒæŸå¤±ï¼š{avg_train_loss:.4f} | BLEUåˆ†æ•°ï¼š{current_bleu:.2f}%")

        return best_bleu

    def calculate_bleu_batch(self, model, src, tgt, src_mask, tokenizer):
        """æ‰¹é‡è®¡ç®—BLEUåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼Œé€‚é…æ¶ˆèå®éªŒï¼‰"""
        batch_size = src.shape[0]

        # 1. ç¼–ç æºåºåˆ—
        memory = model.encode(src, src_mask)

        # 2. ç”Ÿæˆç›®æ ‡åºåˆ—
        tgt_indices = torch.ones(batch_size, 1).fill_(1).long().to(self.device)  # <sos>
        for _ in range(50):  # æœ€å¤§ç”Ÿæˆé•¿åº¦50
            tgt_mask = create_masks(tgt_indices, tgt_indices)[1]
            output = model.decode(tgt_indices, memory, tgt_mask)
            next_word = output[:, -1].argmax(dim=-1)
            tgt_indices = torch.cat([tgt_indices, next_word.unsqueeze(1)], dim=1)
            if (next_word == 2).all():  # æ‰€æœ‰åºåˆ—ç”Ÿæˆ<eos>ï¼Œæå‰åœæ­¢
                break

        # 3. è§£ç å¹¶è®¡ç®—BLEUåˆ†æ•°ï¼ˆ1-gramåŒ¹é…ç‡ï¼‰
        total_bleu = 0.0
        valid_count = 0
        for i in range(batch_size):
            pred_text = tokenizer.decode_tgt(tgt_indices[i].cpu())
            true_text = tokenizer.decode_tgt(tgt[i].cpu())

            pred_words = pred_text.split()
            true_words = true_text.split()

            if len(pred_words) == 0 or len(true_words) == 0:
                continue

            # 1-gramåŒ¹é…æ•°
            matches = len(set(pred_words) & set(true_words))
            precision = matches / len(pred_words)
            total_bleu += precision * 100
            valid_count += 1

        return total_bleu / valid_count if valid_count > 0 else 0.0

    def save_results(self):
        """ä¿å­˜æ¶ˆèå®éªŒç»“æœï¼ˆCSVæ ¼å¼ï¼Œæ–¹ä¾¿åç»­åˆ†æï¼‰"""
        # åˆ›å»ºç»“æœç›®å½•ï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
        os.makedirs("./results", exist_ok=True)

        # è½¬æ¢ç»“æœä¸ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(self.results)
        csv_path = os.path.join("./results", "ablation_results.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"\nğŸ“Š æ¶ˆèå®éªŒç»“æœå·²ä¿å­˜è‡³ï¼š{csv_path}")

        # æ‰“å°ç»“æœè¡¨æ ¼ï¼ˆç›´è§‚æŸ¥çœ‹ï¼‰
        print("\næ¶ˆèå®éªŒç»“æœæ±‡æ€»ï¼š")
        print("=" * 100)
        print(f"{'æ¨¡å‹ç±»å‹':<12} {'å‚æ•°é‡(ç™¾ä¸‡)':<12} {'BLEUåˆ†æ•°(%)':<12} {'d_model':<8} {'nhead':<6} {'å±‚æ•°':<8} {'d_ff':<8} {'dropout':<8}")
        print("-" * 100)
        for result in self.results:
            print(f"{result['model_type']:<12} {result['params_million']:<12.2f} {result['best_bleu']:<12.2f} "
                  f"{result['d_model']:<8} {result['nhead']:<6} {result['num_encoder_layers']:<8} "
                  f"{result['d_ff']:<8} {result['dropout']:<8.1f}")
        print("=" * 100)

    def plot_results(self):
        """ç»˜åˆ¶æ¶ˆèå®éªŒç»“æœå›¾ï¼ˆBLEUåˆ†æ•°å¯¹æ¯”+å‚æ•°é‡å¯¹æ¯”ï¼‰"""
        if not self.results:
            print("âŒ æ— å®éªŒç»“æœï¼Œè·³è¿‡ç»˜å›¾")
            return

        # æå–ç»˜å›¾æ•°æ®
        model_types = [r['model_type'] for r in self.results]
        bleu_scores = [r['best_bleu'] for r in self.results]
        params_million = [r['params_million'] for r in self.results]

        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # 1. BLEUåˆ†æ•°å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
        colors1 = plt.cm.Set3(range(len(model_types)))
        bars1 = ax1.bar(model_types, bleu_scores, color=colors1, alpha=0.8)
        ax1.set_xlabel('æ¨¡å‹é…ç½®', fontsize=12)
        ax1.set_ylabel('BLEUåˆ†æ•° (%)', fontsize=12)
        ax1.set_title('æ¶ˆèå®éªŒ - BLEUåˆ†æ•°å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3, axis='y')

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars1, bleu_scores):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                     f'{score:.2f}%', ha='center', va='bottom', fontweight='bold')

        # 2. å‚æ•°é‡å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
        colors2 = plt.cm.Set2(range(len(model_types)))
        bars2 = ax2.bar(model_types, params_million, color=colors2, alpha=0.8)
        ax2.set_xlabel('æ¨¡å‹é…ç½®', fontsize=12)
        ax2.set_ylabel('å‚æ•°é‡ï¼ˆç™¾ä¸‡ï¼‰', fontsize=12)
        ax2.set_title('æ¶ˆèå®éªŒ - æ¨¡å‹å‚æ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3, axis='y')

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, param in zip(bars2, params_million):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                     f'{param:.2f}M', ha='center', va='bottom', fontweight='bold')

        # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
        plt.tight_layout()
        plot_path = os.path.join("./results", "ablation_study_plots.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… æ¶ˆèå®éªŒå›¾è¡¨å·²ä¿å­˜è‡³ï¼š{plot_path}")

    def print_summary(self):
        """æ‰“å°æ¶ˆèå®éªŒæ€»ç»“ï¼ˆæ‰¾å‡ºæœ€ä¼˜é…ç½®ï¼‰"""
        if not self.results:
            return

        # æ‰¾å‡ºBLEUåˆ†æ•°æœ€é«˜çš„é…ç½®
        best_result = max(self.results, key=lambda x: x['best_bleu'])
        # æ‰¾å‡ºå‚æ•°é‡æœ€å°ä½†BLEUåˆ†æ•°å‰3çš„é…ç½®ï¼ˆå…¼é¡¾æ€§èƒ½å’Œæ•ˆç‡ï¼‰
        efficient_results = sorted(self.results, key=lambda x: (x['best_bleu'], -x['parameters']), reverse=True)[:3]

        print("\nğŸ¯ æ¶ˆèå®éªŒæ€»ç»“")
        print("=" * 70)
        print(f"æœ€ä½³æ€§èƒ½é…ç½®ï¼š{best_result['model_type']}")
        print(f"  - BLEUåˆ†æ•°ï¼š{best_result['best_bleu']:.2f}%")
        print(f"  - å‚æ•°é‡ï¼š{best_result['parameters']:,}ï¼ˆ{best_result['params_million']:.2f}Mï¼‰")
        print(f"  - å…³é”®é…ç½®ï¼šd_model={best_result['d_model']}, nhead={best_result['nhead']}, "
              f"layers={best_result['num_encoder_layers']}, d_ff={best_result['d_ff']}")

        print(f"\né«˜æ•ˆé…ç½®TOP3ï¼ˆæ€§èƒ½-æ•ˆç‡å¹³è¡¡ï¼‰ï¼š")
        for i, res in enumerate(efficient_results, 1):
            print(f"  {i}. {res['model_type']} | BLEUï¼š{res['best_bleu']:.2f}% | å‚æ•°é‡ï¼š{res['params_million']:.2f}M")
        print("=" * 70)


def main():
    """è¿è¡Œæ¶ˆèå®éªŒ"""
    # åˆå§‹åŒ–æ¶ˆèå®éªŒ
    ablation = AblationStudy()
    # è¿è¡Œå®éªŒ
    results = ablation.run_ablation()
    print("\nğŸ‰ æ¶ˆèå®éªŒå…¨éƒ¨å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ ./results ç›®å½•")


if __name__ == "__main__":
    main()